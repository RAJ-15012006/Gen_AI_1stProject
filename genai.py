# -*- coding: utf-8 -*-
"""GenAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iVgOWi9STD1QHKA2uZ6FTwG3PAwBg9ld
"""

# Hugging face, Install and use ollama, Pull a model and talk about langchain
# hugging face is a repo for all this language models
# how to use a existing hugging face model
!pip install transformers

from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline
model_name = "deepset/roberta-base-squad2"

# will ask a question and it need to answer
model = AutoModelForQuestionAnswering.from_pretrained(model_name) # loading a model that existed already, t loads a ready-to-use QA model so you don’t need to train it from scratch.
tokenizer = AutoTokenizer.from_pretrained(model_name) # loading a model that existed already, AutoTokenizer → This is a universal tokenizer loader in transformers.
# It figures out which tokenizer to use (BERT tokenizer, GPT tokenizer, etc.) based on the model.

nlp = pipeline("question-answering", model=model, tokenizer=tokenizer) # tokenizer break into words

corpus = ["2 cups (470 ml) of water",
"2 2.46-ounce (70 g) packets of Maggi",
"1 tablespoon (15 ml) of vegetable oil",
"1 1/2 cups (75 g) of finely diced vegetables, such as carrots, cabbage, or onion",
"1 tablespoon (8 g) of garlic, minced",
"1 to 2 Thai green chilies, chopped",
"2 tablespoons (39 g) of hot sauce, such as schezwan sauce or sriracha sauce",
"1 teaspoon (4.9 ml) of white vinegar",
"2 tablespoons (12 g) of spring onions, chopped",
"Salt, to taste."]

# whatever you give model to train ask questions related to that
while True:
  user_input = input("Ask a question, press q to exit")
  if user_input == "q":
    break
  input_to_model = {"question": user_input, "context":" ".join(corpus)} # question: The user’s question.

# context: The text passage from which the answer should be extracted.
  print ("Answer: ",  nlp(input_to_model)['answer']) # nlp is a Hugging Face pipeline for Question Answering

